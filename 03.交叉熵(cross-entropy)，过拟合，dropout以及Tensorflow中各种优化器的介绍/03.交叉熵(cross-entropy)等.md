## 交叉熵(cross-entropy)，过拟合，dropout以及Tensorflow中各种优化器的介绍 ##

### 一、二次代价函数(quadratic cost) ###

![](https://i.imgur.com/wrAs0CS.png)

其中，C表示代价函数，x表示样本，y表示实际值，a表示输出值，n表示样本的总数。为简单起见
，同样一个样本为例进行说明，此时二次代价函数为：

![](https://i.imgur.com/ihuFuAZ.png)

- a=σ(z), z=∑W j *X j +b
- σ() 是激活函数

假如我们使用梯度下降法(Gradient descent)来调整权值参数的大小，权值w和偏置b的梯度推导
如下：

![](https://i.imgur.com/Qr56rJf.png)

其中，z表示神经元的输入，σ表示激活函数。w和b的梯度跟激活函数的梯度成正比，激活函数的
梯度越大，w和b的大小调整得越快，训练收敛得就越快。

假设我们的激活函数是sigmoid函数：

![](https://i.imgur.com/2DxdyyA.png)

- 假设我们的目标是收敛到1，A点为0.82，距离目标比较远，梯度比较大，权值调整比较大；B点为0.98，距离目标比较近，梯度较小，权值调整比较小。调整方案合理。

- 假设我们的目标是收敛到0，A点为0.82，距离目标比较近，梯度比较大，权值调整比较大；B点为0.98，距离目标比较远，梯度较小，权值调整比较小。调整方案不合理。

### 二、交叉熵代价函数(cross-entropy) ###

换一个思路，我们不改变激活函数，而是改变代价函数，改用交叉熵代价函数：

![](https://i.imgur.com/Qgctf3o.png)

其中，C表示代价函数，x表示样本，y表示实际值，a表示输出值，n表示样本的总数。

![](https://i.imgur.com/UJia2Tr.png)

![](https://i.imgur.com/H3lkeuP.png)

最后得出：

![](https://i.imgur.com/grjI8wu.png)

- 权值和偏置值的调整与![](https://i.imgur.com/EKKr9Qi.png)无关，另外，梯度公式中的![](https://i.imgur.com/M5FNAbL.png)表示输出值与实际值的误差。所以当误差越大时，梯度就越大，参数w和b的调整就越快，训练的速度也就越快。

- 如果输出神经元是线性的，那么二次代价函数就是一种合适的选择。如果输出神经元是S型函数，那么比较适合用交叉熵代价函数。

### 三、对数释然代价函数(log-likelihood cost) ###

- 对数释然函数常用来作为softmax回归的代价函数，如果输出层神经元是sigmoid函数，可以采用
交叉熵代价函数。而深度学习中更普遍的做法是将softmax作为最后一层，此时常用的代价函数是
对数释然代价函数。

- 对数似然代价函数与softmax的组合和交叉熵与sigmoid函数的组合非常相似。对数释然代价函数
在二分类时可以化简为交叉熵代价函数的形式。

在Tensorflow中用：

	tf.nn.sigmoid_cross_entropy_with_logits()来表示跟sigmoid搭配使用的交叉熵。

	tf.nn.softmax_cross_entropy_with_logits()来表示跟softmax搭配使用的交叉熵。

测试如下：

	# coding: utf-8
	
	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	# 载入数据集
	mnist = input_data.read_data_sets("MNIST_data", one_hot=True)
	
	# 每个批次的大小
	batch_size = 100
	# 计算一共有多少个批次
	n_batch = mnist.train.num_examples // batch_size
	
	# 定义两个placeholder
	x = tf.placeholder(tf.float32, [None, 784])
	y = tf.placeholder(tf.float32, [None, 10])
	
	# 创建一个简单的神经网络
	W = tf.Variable(tf.zeros([784, 10]))
	b = tf.Variable(tf.zeros([10]))
	prediction = tf.nn.softmax(tf.matmul(x, W) + b)
	
	# 二次代价函数
	# loss = tf.reduce_mean(tf.square(y-prediction))
	# 这里使用对数释然代价函数tf.nn.softmax_cross_entropy_with_logits()来表示跟softmax搭配使用的交叉熵
	loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))
	# 使用梯度下降法
	train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)
	
	# 初始化变量
	init = tf.global_variables_initializer()
	
	# 结果存放在一个布尔型列表中
	correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))  # argmax返回一维张量中最大的值所在的位置
	# 求准确率
	accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
	
	with tf.Session() as sess:
	    sess.run(init)
	    for epoch in range(21):
	        for batch in range(n_batch):
	            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
	            sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys})
	
	        acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})
	        print("Iter " + str(epoch) + ",Testing Accuracy " + str(acc))


运行结果如下：

	Extracting MNIST_data\train-images-idx3-ubyte.gz
	Extracting MNIST_data\train-labels-idx1-ubyte.gz
	Extracting MNIST_data\t10k-images-idx3-ubyte.gz
	Extracting MNIST_data\t10k-labels-idx1-ubyte.gz
	Iter 0,Testing Accuracy 0.8254
	Iter 1,Testing Accuracy 0.8934
	Iter 2,Testing Accuracy 0.9021
	Iter 3,Testing Accuracy 0.9046
	Iter 4,Testing Accuracy 0.9082
	Iter 5,Testing Accuracy 0.9111
	Iter 6,Testing Accuracy 0.9123
	Iter 7,Testing Accuracy 0.9137
	Iter 8,Testing Accuracy 0.9146
	Iter 9,Testing Accuracy 0.9158
	Iter 10,Testing Accuracy 0.9169
	Iter 11,Testing Accuracy 0.9176
	Iter 12,Testing Accuracy 0.9188
	Iter 13,Testing Accuracy 0.9198
	Iter 14,Testing Accuracy 0.9199
	Iter 15,Testing Accuracy 0.9204
	Iter 16,Testing Accuracy 0.92
	Iter 17,Testing Accuracy 0.9208
	Iter 18,Testing Accuracy 0.9212
	Iter 19,Testing Accuracy 0.9221
	Iter 20,Testing Accuracy 0.9214

下面将对比使用二次代价函数：

	对softmax使用二次代价函数结果		    对softmax使用对数释然函数结果
	
	Iter 0,Testing Accuracy 0.8347		Iter 0,Testing Accuracy 0.8254
	Iter 1,Testing Accuracy 0.8709      Iter 1,Testing Accuracy 0.8934
	Iter 2,Testing Accuracy 0.8822      Iter 2,Testing Accuracy 0.9021
	Iter 3,Testing Accuracy 0.888       Iter 3,Testing Accuracy 0.9046
	Iter 4,Testing Accuracy 0.8937      Iter 4,Testing Accuracy 0.9082
	Iter 5,Testing Accuracy 0.8972      Iter 5,Testing Accuracy 0.9111
	Iter 6,Testing Accuracy 0.9004      Iter 6,Testing Accuracy 0.9123
	Iter 7,Testing Accuracy 0.9016      Iter 7,Testing Accuracy 0.9137
	Iter 8,Testing Accuracy 0.9033      Iter 8,Testing Accuracy 0.9146
	Iter 9,Testing Accuracy 0.9047      Iter 9,Testing Accuracy 0.9158
	Iter 10,Testing Accuracy 0.9067     Iter 10,Testing Accuracy 0.9169
	Iter 11,Testing Accuracy 0.9075     Iter 11,Testing Accuracy 0.9176
	Iter 12,Testing Accuracy 0.9082     Iter 12,Testing Accuracy 0.9188
	Iter 13,Testing Accuracy 0.9093     Iter 13,Testing Accuracy 0.9198
	Iter 14,Testing Accuracy 0.9107     Iter 14,Testing Accuracy 0.9199
	Iter 15,Testing Accuracy 0.9109     Iter 15,Testing Accuracy 0.9204
	Iter 16,Testing Accuracy 0.9118     Iter 16,Testing Accuracy 0.92
	Iter 17,Testing Accuracy 0.9132     Iter 17,Testing Accuracy 0.9208
	Iter 18,Testing Accuracy 0.913      Iter 18,Testing Accuracy 0.9212
	Iter 19,Testing Accuracy 0.9134     Iter 19,Testing Accuracy 0.9221
	Iter 20,Testing Accuracy 0.9131     Iter 20,Testing Accuracy 0.9214

可见，使用对数释然函数训练更快。

### 四、拟合 ###

![](https://i.imgur.com/m21PNLD.png)

![](https://i.imgur.com/QqIZBqK.png)

防止过拟合：

- 增加数据集

一般来说，更多的数据参与训练得到的模型就越好，如果数据太少，而我们构建的神经网络又太复杂，节点很多的话就比较容易产生过拟合的现象。

- 正则化方法 ：

![](https://i.imgur.com/utPWXJT.png)  

正则化方法是指在进行代价函数优化时，在代价函数后面加上一个正则项，这个正则项是跟权值相关的。入正则项系数，权衡正则项与C0的比重，n是训练集样本的大小，它会使得原先那些处于0附件的权值往0移动，从而降低模型的复杂度，防止过拟合。

- Dropout：

![](https://i.imgur.com/T8ZsvhZ.png)

正则式通过在代价函数后面追加正则项来防止过度拟合的，还有一个方法是通过修改神经元本身的机构来实现的，称为Dropout。该方法是对神经网络进行训练时用到的一种技巧。

下面这个程序将演示过拟合和Dropout的效果。

还是以手写数字识别为例，与以前不同的是，这里构建的网络更加复杂，这里构建的输入层网络有784个神经元，第一个隐藏层有2000个神经元，第二个隐藏层有2000个神经元，第三个隐藏层有1000个神经元，输出层为10个神经元，这里的测试环境使用GPU版本，CPU版本训练时间会过长。第一轮实验使用所有的神经元进行训练：

	sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.0})

第二轮实验使用70%的神经元进行测试，也就是Dropout：

	sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 0.7})

训练的代码如下：

	# coding: utf-8
	
	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	# 载入数据集
	mnist = input_data.read_data_sets("MNIST_data", one_hot=True)
	
	# 每个批次的大小
	batch_size = 100
	# 计算一共有多少个批次
	n_batch = mnist.train.num_examples // batch_size
	
	# 定义两个placeholder
	x = tf.placeholder(tf.float32, [None, 784])
	y = tf.placeholder(tf.float32, [None, 10])
	keep_prob = tf.placeholder(tf.float32)
	
	# 创建一个简单的神经网络
	W1 = tf.Variable(tf.truncated_normal([784, 2000], stddev=0.1))
	b1 = tf.Variable(tf.zeros([2000]) + 0.1)
	L1 = tf.nn.tanh(tf.matmul(x, W1) + b1)
	L1_drop = tf.nn.dropout(L1, keep_prob)
	
	W2 = tf.Variable(tf.truncated_normal([2000, 2000], stddev=0.1))
	b2 = tf.Variable(tf.zeros([2000]) + 0.1)
	L2 = tf.nn.tanh(tf.matmul(L1_drop, W2) + b2)
	L2_drop = tf.nn.dropout(L2, keep_prob)
	
	W3 = tf.Variable(tf.truncated_normal([2000, 1000], stddev=0.1))
	b3 = tf.Variable(tf.zeros([1000]) + 0.1)
	L3 = tf.nn.tanh(tf.matmul(L2_drop, W3) + b3)
	L3_drop = tf.nn.dropout(L3, keep_prob)
	
	W4 = tf.Variable(tf.truncated_normal([1000, 10], stddev=0.1))
	b4 = tf.Variable(tf.zeros([10]) + 0.1)
	prediction = tf.nn.softmax(tf.matmul(L3_drop, W4) + b4)
	
	# 二次代价函数
	# loss = tf.reduce_mean(tf.square(y-prediction))
	loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))
	# 使用梯度下降法
	train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)
	
	# 初始化变量
	init = tf.global_variables_initializer()
	
	# 结果存放在一个布尔型列表中
	correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))  # argmax返回一维张量中最大的值所在的位置
	# 求准确率
	accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
	
	with tf.Session() as sess:
	    sess.run(init)
	    for epoch in range(31):
	        for batch in range(n_batch):
	            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
	            sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.0})
	
	        test_acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0})
	        train_acc = sess.run(accuracy, feed_dict={x: mnist.train.images, y: mnist.train.labels, keep_prob: 1.0})
	        print("Iter " + str(epoch) + ",Testing Accuracy " + str(test_acc) + ",Training Accuracy " + str(train_acc))


使用全部神经元训练过程如下：

	Extracting MNIST_data\train-images-idx3-ubyte.gz
	Extracting MNIST_data\train-labels-idx1-ubyte.gz
	Extracting MNIST_data\t10k-images-idx3-ubyte.gz
	Extracting MNIST_data\t10k-labels-idx1-ubyte.gz
	2018-08-06 10:53:24.388643: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
	2018-08-06 10:53:25.040368: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1030] Found device 0 with properties:
	name: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 0.8605
	pciBusID: 0000:01:00.0
	totalMemory: 2.00GiB freeMemory: 1.65GiB
	2018-08-06 10:53:25.052133: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)
	Iter 0,Testing Accuracy 0.9509,Training Accuracy 0.95814544
	Iter 1,Testing Accuracy 0.9584,Training Accuracy 0.9743636
	Iter 2,Testing Accuracy 0.9634,Training Accuracy 0.9816909
	Iter 3,Testing Accuracy 0.9697,Training Accuracy 0.986
	Iter 4,Testing Accuracy 0.9704,Training Accuracy 0.9881455
	Iter 5,Testing Accuracy 0.972,Training Accuracy 0.9893818
	Iter 6,Testing Accuracy 0.9717,Training Accuracy 0.99056363
	Iter 7,Testing Accuracy 0.9725,Training Accuracy 0.9912182
	Iter 8,Testing Accuracy 0.9726,Training Accuracy 0.9918727
	Iter 9,Testing Accuracy 0.9733,Training Accuracy 0.9926
	Iter 10,Testing Accuracy 0.9728,Training Accuracy 0.99323636
	Iter 11,Testing Accuracy 0.9737,Training Accuracy 0.99356365
	Iter 12,Testing Accuracy 0.9724,Training Accuracy 0.99381816
	Iter 13,Testing Accuracy 0.9731,Training Accuracy 0.9940364
	Iter 14,Testing Accuracy 0.9736,Training Accuracy 0.99414545
	Iter 15,Testing Accuracy 0.9732,Training Accuracy 0.9944
	Iter 16,Testing Accuracy 0.974,Training Accuracy 0.9945273
	Iter 17,Testing Accuracy 0.9742,Training Accuracy 0.9946727
	Iter 18,Testing Accuracy 0.9732,Training Accuracy 0.9947818
	Iter 19,Testing Accuracy 0.9726,Training Accuracy 0.9948909
	Iter 20,Testing Accuracy 0.973,Training Accuracy 0.99503636
	Iter 21,Testing Accuracy 0.973,Training Accuracy 0.99512726
	Iter 22,Testing Accuracy 0.9737,Training Accuracy 0.99538183
	Iter 23,Testing Accuracy 0.9742,Training Accuracy 0.99545455
	Iter 24,Testing Accuracy 0.9738,Training Accuracy 0.9955091
	Iter 25,Testing Accuracy 0.9745,Training Accuracy 0.9955091
	Iter 26,Testing Accuracy 0.9745,Training Accuracy 0.9955636
	Iter 27,Testing Accuracy 0.9747,Training Accuracy 0.99563634
	Iter 28,Testing Accuracy 0.975,Training Accuracy 0.9956545
	Iter 29,Testing Accuracy 0.9751,Training Accuracy 0.99587274
	Iter 30,Testing Accuracy 0.9744,Training Accuracy 0.99587274

Dropout后：

	Extracting MNIST_data\train-images-idx3-ubyte.gz
	Extracting MNIST_data\train-labels-idx1-ubyte.gz
	Extracting MNIST_data\t10k-images-idx3-ubyte.gz
	Extracting MNIST_data\t10k-labels-idx1-ubyte.gz
	2018-08-06 10:45:16.874624: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
	2018-08-06 10:45:17.375634: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1030] Found device 0 with properties:
	name: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 0.8605
	pciBusID: 0000:01:00.0
	totalMemory: 2.00GiB freeMemory: 1.65GiB
	2018-08-06 10:45:17.386446: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)
	Iter 0,Testing Accuracy 0.9186,Training Accuracy 0.9130545
	Iter 1,Testing Accuracy 0.9309,Training Accuracy 0.92783636
	Iter 2,Testing Accuracy 0.936,Training Accuracy 0.9354727
	Iter 3,Testing Accuracy 0.9419,Training Accuracy 0.94101816
	Iter 4,Testing Accuracy 0.9439,Training Accuracy 0.94461817
	Iter 5,Testing Accuracy 0.9461,Training Accuracy 0.94836366
	Iter 6,Testing Accuracy 0.9515,Training Accuracy 0.9513636
	Iter 7,Testing Accuracy 0.9533,Training Accuracy 0.9545091
	Iter 8,Testing Accuracy 0.9531,Training Accuracy 0.95625454
	Iter 9,Testing Accuracy 0.9555,Training Accuracy 0.95861816
	Iter 10,Testing Accuracy 0.9568,Training Accuracy 0.96050906
	Iter 11,Testing Accuracy 0.957,Training Accuracy 0.9624182
	Iter 12,Testing Accuracy 0.9601,Training Accuracy 0.9635091
	Iter 13,Testing Accuracy 0.9615,Training Accuracy 0.9640727
	Iter 14,Testing Accuracy 0.9626,Training Accuracy 0.9658545
	Iter 15,Testing Accuracy 0.9621,Training Accuracy 0.9670182
	Iter 16,Testing Accuracy 0.9645,Training Accuracy 0.9676909
	Iter 17,Testing Accuracy 0.9665,Training Accuracy 0.9687091
	Iter 18,Testing Accuracy 0.964,Training Accuracy 0.9690545
	Iter 19,Testing Accuracy 0.9669,Training Accuracy 0.97034544
	Iter 20,Testing Accuracy 0.9676,Training Accuracy 0.97130907
	Iter 21,Testing Accuracy 0.966,Training Accuracy 0.9718
	Iter 22,Testing Accuracy 0.9673,Training Accuracy 0.9727273
	Iter 23,Testing Accuracy 0.9681,Training Accuracy 0.9733818
	Iter 24,Testing Accuracy 0.9682,Training Accuracy 0.97481817
	Iter 25,Testing Accuracy 0.9688,Training Accuracy 0.97481817
	Iter 26,Testing Accuracy 0.9695,Training Accuracy 0.9764909
	Iter 27,Testing Accuracy 0.9697,Training Accuracy 0.97636366
	Iter 28,Testing Accuracy 0.9698,Training Accuracy 0.97705454
	Iter 29,Testing Accuracy 0.97,Training Accuracy 0.9773818
	Iter 30,Testing Accuracy 0.9706,Training Accuracy 0.97776365

可以对比两种训练方式最后的五次训练结果：

	# 使用所有神经元：
	Iter 26,Testing Accuracy 0.9745,Training Accuracy 0.9955636
	Iter 27,Testing Accuracy 0.9747,Training Accuracy 0.99563634
	Iter 28,Testing Accuracy 0.975,Training Accuracy 0.9956545
	Iter 29,Testing Accuracy 0.9751,Training Accuracy 0.99587274
	Iter 30,Testing Accuracy 0.9744,Training Accuracy 0.99587274

	# Dropout:
	Iter 26,Testing Accuracy 0.9695,Training Accuracy 0.9764909
	Iter 27,Testing Accuracy 0.9697,Training Accuracy 0.97636366
	Iter 28,Testing Accuracy 0.9698,Training Accuracy 0.97705454
	Iter 29,Testing Accuracy 0.97,Training Accuracy 0.9773818
	Iter 30,Testing Accuracy 0.9706,Training Accuracy 0.97776365

从上面可以看到，不使用Dropout方法训练后训练集测试网络准确率约99%，测试集则为97%，而且可以看出这个结果一直保持了很久，即使训练持续进行，这便是过拟合了。然而使用了Dropout方法训练后，测试集和训练集最后测试网络得到的结果基本差异不大，拟合度较高。

### 五、Optimizer优化器 ###

Tensorflow提供了下面这几种优化器：

- tf.train.GradientDescentOptimizer
- tf.train.AdadeltaOptimizer
- tf.train.AdagradOptimizer
- tf.train.AdagradDAOptimizer
- tf.train.MomentumOptimizer
- tf.train.AdamOptimizer
- tf.train.FtrlOptimizer
- tf.train.ProximalGradientDescentOptimizer
- tf.train.ProximalAdagradOptimizer
- tf.train.RMSPropOptimizer

各种优化器对比：

- 标准梯度下降法：标准梯度下降先计算所有样本汇总误差，然后根据总误差来更新权值
- 随机梯度下降法：随机梯度下降随机抽取一个样本来计算误差，然后更新权值
- 批量梯度下降法：批量梯度下降算是一种折中的方案，从总样本中选取一个批次（比如一共有10000个样本，随机选取100个样本作为一个batch），然后计算这个batch的总误差，根据总误差来更新权值。

![](https://i.imgur.com/kgQ0aBF.png)

其中：

	W： 要训练的参数
	J(W)： 代价函数
	∇ W J(W)： 代价函数的梯度
	η： 学习率

#### SGD：####

![](https://i.imgur.com/QAQRWdV.png)

#### Momentum：####

![](https://i.imgur.com/i69AHZ0.png)

当前权值的改变会受到上一次权值改变的影响，类似于小球向下滚动的时候带上了惯性。这样
可以加快小球的向下的速度。

#### NAG（Nesterov accelerated gradient）：####

![](https://i.imgur.com/iVohT7g.png)

NAG在TF中跟Momentum合并在同一个函数tf.train.MomentumOptimizer中，可以通过参
数配置启用。
在Momentun中小球会盲目地跟从下坡的梯度，容易发生错误，所以我们需要一个更聪明的
小球，这个小球提前知道它要去哪里，它还要知道走到坡底的时候速度慢下来而不是又冲上另
一个坡。γvt−1会用来修改W的值，计算W−γvt−1可以表示小球下一个位置大概在哪里。从
而我们可以提前计算下一个位置的梯度，然后使用到当前位置。

#### Adagrad：####

![](https://i.imgur.com/qsF6ErD.png)

它是基于SGD的一种算法，它的核心思想是对比较常见的数据给予它比较小的学习率去调整
参数，对于比较罕见的数据给予它比较大的学习率去调整参数。它很适合应用于数据稀疏的数
据集（比如一个图片数据集，有10000张狗的照片，10000张猫的照片，只有100张大象的照
片）。

Adagrad主要的优势在于不需要人为的调节学习率，它可以自动调节。它的缺点在于，随着
迭代次数的增多，学习率也会越来越低，最终会趋向于0。

#### RMSprop：####

RMS（Root Mean Square）是均方根的缩写。

![](https://i.imgur.com/u2Ew5m3.png)

RMSprop借鉴了一些Adagrad的思想，不过这里RMSprop只用到了前t-1次梯度平方的平均值加上当前梯度的平方的和的开平方作为学习率的分母。这样RMSprop不会出现学习率越来
越低的问题，而且也能自己调节学习率，并且可以有一个比较好的效果。

#### Adadelta：####

![](https://i.imgur.com/zzEDF3w.png)

使用Adadelta我们甚至不需要设置一个默认学习率，在Adadelta不需要使用学习率也可以达
到一个非常好的效果。

#### Adam：####

![](https://i.imgur.com/VRrJ07P.png)

就像Adadelta和RMSprop一样Adam会存储之前衰减的平方梯度，同时它也会保存之前衰减
的梯度。经过一些处理之后再使用类似Adadelta和RMSprop的方式更新参数。

下面使用tf.train.AdadeltaOptimizer来训练手写数字：

	# coding: utf-8
	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	# 载入数据集
	mnist = input_data.read_data_sets("MNIST_data", one_hot=True)
	
	# 每个批次的大小
	batch_size = 100
	# 计算一共有多少个批次
	n_batch = mnist.train.num_examples // batch_size
	
	# 定义两个placeholder
	x = tf.placeholder(tf.float32, [None, 784])
	y = tf.placeholder(tf.float32, [None, 10])
	
	# 创建一个简单的神经网络
	W = tf.Variable(tf.zeros([784, 10]))
	b = tf.Variable(tf.zeros([10]))
	prediction = tf.nn.softmax(tf.matmul(x, W) + b)
	
	# 二次代价函数
	# loss = tf.reduce_mean(tf.square(y-prediction))
	loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))
	# 使用AdamOptimizer
	train_step = tf.train.AdamOptimizer(1e-2).minimize(loss)
	
	# 初始化变量
	init = tf.global_variables_initializer()
	
	# 结果存放在一个布尔型列表中
	correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))  # argmax返回一维张量中最大的值所在的位置
	# 求准确率
	accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
	
	with tf.Session() as sess:
	    sess.run(init)
	    for epoch in range(21):
	        for batch in range(n_batch):
	            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
	            sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys})
	
	        acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})
	        print("Iter " + str(epoch) + ",Testing Accuracy " + str(acc))

训练过程如下：

	Iter 0,Testing Accuracy 0.9219
	Iter 1,Testing Accuracy 0.9248
	Iter 2,Testing Accuracy 0.9298
	Iter 3,Testing Accuracy 0.9298
	Iter 4,Testing Accuracy 0.9265
	Iter 5,Testing Accuracy 0.9289
	Iter 6,Testing Accuracy 0.9282
	Iter 7,Testing Accuracy 0.9312
	Iter 8,Testing Accuracy 0.9283
	Iter 9,Testing Accuracy 0.9296
	Iter 10,Testing Accuracy 0.9276
	Iter 11,Testing Accuracy 0.9334
	Iter 12,Testing Accuracy 0.933
	Iter 13,Testing Accuracy 0.9302
	Iter 14,Testing Accuracy 0.9297
	Iter 15,Testing Accuracy 0.9302
	Iter 16,Testing Accuracy 0.9317
	Iter 17,Testing Accuracy 0.9332
	Iter 18,Testing Accuracy 0.9347
	Iter 19,Testing Accuracy 0.9321
	Iter 20,Testing Accuracy 0.9319

### 六、提高准确度 ###

构建更加复杂的网络，训练更多次数来提高训练的准确度：

	# coding: utf-8
	
	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	# 载入数据集
	mnist = input_data.read_data_sets("MNIST_data", one_hot=True)
	
	# 每个批次的大小
	batch_size = 100
	# 计算一共有多少个批次
	n_batch = mnist.train.num_examples // batch_size
	
	# 定义两个placeholder
	x = tf.placeholder(tf.float32, [None, 784])
	y = tf.placeholder(tf.float32, [None, 10])
	keep_prob = tf.placeholder(tf.float32)
	
	# 创建一个简单的神经网络
	W1 = tf.Variable(tf.truncated_normal([784, 1000], stddev=0.1))
	b1 = tf.Variable(tf.zeros([1000]) + 0.1)
	L1 = tf.nn.tanh(tf.matmul(x, W1) + b1)
	L1_drop = tf.nn.dropout(L1, keep_prob)
	
	W2 = tf.Variable(tf.truncated_normal([1000, 500], stddev=0.1))
	b2 = tf.Variable(tf.zeros([500]) + 0.1)
	L2 = tf.nn.tanh(tf.matmul(L1_drop, W2) + b2)
	L2_drop = tf.nn.dropout(L2, keep_prob)
	
	W3 = tf.Variable(tf.truncated_normal([500, 200], stddev=0.1))
	b3 = tf.Variable(tf.zeros([200]) + 0.1)
	L3 = tf.nn.tanh(tf.matmul(L2_drop, W3) + b3)
	L3_drop = tf.nn.dropout(L3, keep_prob)
	
	W4 = tf.Variable(tf.truncated_normal([200, 10], stddev=0.1))
	b4 = tf.Variable(tf.zeros([10]) + 0.1)
	prediction = tf.nn.softmax(tf.matmul(L3_drop, W4) + b4)
	
	# 二次代价函数
	# loss = tf.reduce_mean(tf.square(y-prediction))
	loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))
	# 使用梯度下降法
	train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)
	
	# 初始化变量
	init = tf.global_variables_initializer()
	
	# 结果存放在一个布尔型列表中
	correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))  # argmax返回一维张量中最大的值所在的位置
	# 求准确率
	accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
	
	with tf.Session() as sess:
	    sess.run(init)
	    for epoch in range(50):
	        for batch in range(n_batch):
	            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
	            sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 0.7})
	
	        test_acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0})
	        train_acc = sess.run(accuracy, feed_dict={x: mnist.train.images, y: mnist.train.labels, keep_prob: 1.0})
	        print("Iter " + str(epoch) + ",Testing Accuracy " + str(test_acc) + ",Training Accuracy " + str(train_acc))

训练过程如下：

	Extracting MNIST_data\train-images-idx3-ubyte.gz
	Extracting MNIST_data\train-labels-idx1-ubyte.gz
	Extracting MNIST_data\t10k-images-idx3-ubyte.gz
	Extracting MNIST_data\t10k-labels-idx1-ubyte.gz
	2018-08-06 14:29:48.651411: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
	2018-08-06 14:29:48.972187: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1030] Found device 0 with properties:
	name: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 0.8605
	pciBusID: 0000:01:00.0
	totalMemory: 2.00GiB freeMemory: 1.65GiB
	2018-08-06 14:29:48.983929: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)
	Iter 0,Testing Accuracy 0.915,Training Accuracy 0.90909094
	Iter 1,Testing Accuracy 0.9269,Training Accuracy 0.9244182
	Iter 2,Testing Accuracy 0.9365,Training Accuracy 0.9355636
	Iter 3,Testing Accuracy 0.9382,Training Accuracy 0.9404
	Iter 4,Testing Accuracy 0.9449,Training Accuracy 0.9466364
	Iter 5,Testing Accuracy 0.9458,Training Accuracy 0.94985455
	Iter 6,Testing Accuracy 0.948,Training Accuracy 0.9532
	Iter 7,Testing Accuracy 0.9516,Training Accuracy 0.955
	Iter 8,Testing Accuracy 0.9542,Training Accuracy 0.9572727
	Iter 9,Testing Accuracy 0.9555,Training Accuracy 0.9601455
	Iter 10,Testing Accuracy 0.9556,Training Accuracy 0.9614909
	Iter 11,Testing Accuracy 0.9571,Training Accuracy 0.9625091
	Iter 12,Testing Accuracy 0.9592,Training Accuracy 0.96434546
	Iter 13,Testing Accuracy 0.9596,Training Accuracy 0.9654
	Iter 14,Testing Accuracy 0.9616,Training Accuracy 0.96694547
	Iter 15,Testing Accuracy 0.9611,Training Accuracy 0.9678
	Iter 16,Testing Accuracy 0.9629,Training Accuracy 0.9690727
	Iter 17,Testing Accuracy 0.9635,Training Accuracy 0.97005457
	Iter 18,Testing Accuracy 0.9661,Training Accuracy 0.9709455
	Iter 19,Testing Accuracy 0.9664,Training Accuracy 0.9720182
	Iter 20,Testing Accuracy 0.9664,Training Accuracy 0.97194546
	Iter 21,Testing Accuracy 0.9662,Training Accuracy 0.97327274
	Iter 22,Testing Accuracy 0.966,Training Accuracy 0.9738727
	Iter 23,Testing Accuracy 0.9682,Training Accuracy 0.97401816
	Iter 24,Testing Accuracy 0.9688,Training Accuracy 0.97523636
	Iter 25,Testing Accuracy 0.9681,Training Accuracy 0.9756727
	Iter 26,Testing Accuracy 0.9686,Training Accuracy 0.97614545
	Iter 27,Testing Accuracy 0.9698,Training Accuracy 0.9762727
	Iter 28,Testing Accuracy 0.9695,Training Accuracy 0.9770909
	Iter 29,Testing Accuracy 0.9697,Training Accuracy 0.97761816
	Iter 30,Testing Accuracy 0.9704,Training Accuracy 0.97794545
	Iter 31,Testing Accuracy 0.97,Training Accuracy 0.97825456
	Iter 32,Testing Accuracy 0.9705,Training Accuracy 0.9792
	Iter 33,Testing Accuracy 0.9706,Training Accuracy 0.97925454
	Iter 34,Testing Accuracy 0.9708,Training Accuracy 0.9798727
	Iter 35,Testing Accuracy 0.971,Training Accuracy 0.98032725
	Iter 36,Testing Accuracy 0.971,Training Accuracy 0.98063636
	Iter 37,Testing Accuracy 0.9711,Training Accuracy 0.9810909
	Iter 38,Testing Accuracy 0.9721,Training Accuracy 0.98147273
	Iter 39,Testing Accuracy 0.972,Training Accuracy 0.9816545
	Iter 40,Testing Accuracy 0.972,Training Accuracy 0.9818364
	Iter 41,Testing Accuracy 0.9727,Training Accuracy 0.98274547
	Iter 42,Testing Accuracy 0.973,Training Accuracy 0.98261815
	Iter 43,Testing Accuracy 0.973,Training Accuracy 0.98274547
	Iter 44,Testing Accuracy 0.9724,Training Accuracy 0.9830545
	Iter 45,Testing Accuracy 0.9745,Training Accuracy 0.98343635
	Iter 46,Testing Accuracy 0.9741,Training Accuracy 0.98398185
	Iter 47,Testing Accuracy 0.9734,Training Accuracy 0.98407274
	Iter 48,Testing Accuracy 0.9731,Training Accuracy 0.9844
	Iter 49,Testing Accuracy 0.9739,Training Accuracy 0.9844364