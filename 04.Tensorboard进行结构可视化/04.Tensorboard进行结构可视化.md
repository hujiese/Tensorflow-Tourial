## Tensorboard进行结构可视化 ##

### 下面这段代码演示了如何让04.Tensorboard进行结构可视化： ###

	# coding: utf-8
	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	# 载入数据集
	mnist = input_data.read_data_sets("MNIST_data", one_hot=True)
	
	# 每个批次的大小
	batch_size = 100
	# 计算一共有多少个批次
	n_batch = mnist.train.num_examples // batch_size
	
	# 命名空间
	with tf.name_scope('input'):
	    # 定义两个placeholder
	    x = tf.placeholder(tf.float32, [None, 784], name='x-input')
	    y = tf.placeholder(tf.float32, [None, 10], name='y-input')
	
	with tf.name_scope('layer'):
	    # 创建一个简单的神经网络
	    with tf.name_scope('wights'):
	        W = tf.Variable(tf.zeros([784, 10]), name='W')
	    with tf.name_scope('biases'):
	        b = tf.Variable(tf.zeros([10]), name='b')
	    with tf.name_scope('wx_plus_b'):
	        wx_plus_b = tf.matmul(x, W) + b
	    with tf.name_scope('softmax'):
	        prediction = tf.nn.softmax(wx_plus_b)
	
	# 二次代价函数
	# loss = tf.reduce_mean(tf.square(y-prediction))
	with tf.name_scope('loss'):
	    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))
	with tf.name_scope('train'):
	    # 使用梯度下降法
	    train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)
	
	# 初始化变量
	init = tf.global_variables_initializer()
	
	with tf.name_scope('accuracy'):
	    with tf.name_scope('correct_prediction'):
	        # 结果存放在一个布尔型列表中
	        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))  # argmax返回一维张量中最大的值所在的位置
	    with tf.name_scope('accuracy'):
	        # 求准确率
	        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
	
	with tf.Session() as sess:
	    sess.run(init)
	    writer = tf.summary.FileWriter('logs/', sess.graph)
	    for epoch in range(1):
	        for batch in range(n_batch):
	            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
	            sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys})
	
	        acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})
	        print("Iter " + str(epoch) + ",Testing Accuracy " + str(acc))

通过使用

	with tf.name_scope('input')

来设置命名空间标记可视化参数，程序运行之后将在当前目录生成一个logs目录，目录下有如下内容：

![](https://i.imgur.com/SJ88lgA.png)

然后运行下面这个命令：

	tensorboard --logdir=F:\Tensorflow\day05\logs

例如：

![](https://i.imgur.com/8N00A8K.png)

打开浏览器，输入：
> http://DESKTOP-DVA2NMJ:6006

可以看到：

![](https://i.imgur.com/8dyTEES.png)

然后可以点击观察里面的一些细节：

![](https://i.imgur.com/vIeCYc9.png)

### 下面这个例子演示了如何让参数细节可视化，将绘制各个参数的变化情况： ###

	# coding: utf-8
	
	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	
	# 载入数据集
	mnist = input_data.read_data_sets("MNIST_data", one_hot=True)
	
	# 每个批次的大小
	batch_size = 100
	# 计算一共有多少个批次
	n_batch = mnist.train.num_examples // batch_size
	
	
	# 参数概要
	def variable_summaries(var):
	    with tf.name_scope('summaries'):
	        mean = tf.reduce_mean(var)
	        tf.summary.scalar('mean', mean)  # 平均值
	        with tf.name_scope('stddev'):
	            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
	        tf.summary.scalar('stddev', stddev)  # 标准差
	        tf.summary.scalar('max', tf.reduce_max(var))  # 最大值
	        tf.summary.scalar('min', tf.reduce_min(var))  # 最小值
	        tf.summary.histogram('histogram', var)  # 直方图
	
	
	# 命名空间
	with tf.name_scope('input'):
	    # 定义两个placeholder
	    x = tf.placeholder(tf.float32, [None, 784], name='x-input')
	    y = tf.placeholder(tf.float32, [None, 10], name='y-input')
	
	with tf.name_scope('layer'):
	    # 创建一个简单的神经网络
	    with tf.name_scope('wights'):
	        W = tf.Variable(tf.zeros([784, 10]), name='W')
	        variable_summaries(W)
	    with tf.name_scope('biases'):
	        b = tf.Variable(tf.zeros([10]), name='b')
	        variable_summaries(b)
	    with tf.name_scope('wx_plus_b'):
	        wx_plus_b = tf.matmul(x, W) + b
	    with tf.name_scope('softmax'):
	        prediction = tf.nn.softmax(wx_plus_b)
	
	# 二次代价函数
	# loss = tf.reduce_mean(tf.square(y-prediction))
	with tf.name_scope('loss'):
	    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))
	    tf.summary.scalar('loss', loss)
	with tf.name_scope('train'):
	    # 使用梯度下降法
	    train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)
	
	# 初始化变量
	init = tf.global_variables_initializer()
	
	with tf.name_scope('accuracy'):
	    with tf.name_scope('correct_prediction'):
	        # 结果存放在一个布尔型列表中
	        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))  # argmax返回一维张量中最大的值所在的位置
	    with tf.name_scope('accuracy'):
	        # 求准确率
	        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
	        tf.summary.scalar('accuracy', accuracy)
	
	# 合并所有的summary
	merged = tf.summary.merge_all()
	
	with tf.Session() as sess:
	    sess.run(init)
	    writer = tf.summary.FileWriter('logs/', sess.graph)
	    for epoch in range(51):
	        for batch in range(n_batch):
	            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
	            summary, _ = sess.run([merged, train_step], feed_dict={x: batch_xs, y: batch_ys})
	
	        writer.add_summary(summary, epoch)
	        acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})
	        print("Iter " + str(epoch) + ",Testing Accuracy " + str(acc))

Tensorboard内容大致如下：

![](https://i.imgur.com/507TToT.png)

![](https://i.imgur.com/LlSQEeg.png)

![](https://i.imgur.com/ritUSN8.png)

下面进行手写数字识别Embeding可视化过程：

	# coding: utf-8
	
	import tensorflow as tf
	from tensorflow.examples.tutorials.mnist import input_data
	from tensorflow.contrib.tensorboard.plugins import projector
	
	# 载入数据集
	mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
	# 运行次数
	max_steps = 1001
	# 图片数量
	image_num = 3000
	# 文件路径
	DIR = "F:/Tensorflow/day05/"
	
	# 定义会话
	sess = tf.Session()
	
	# 载入图片
	embedding = tf.Variable(tf.stack(mnist.test.images[:image_num]), trainable=False, name='embedding')
	
	# 参数概要
	def variable_summaries(var):
	    with tf.name_scope('summaries'):
	        mean = tf.reduce_mean(var)
	        tf.summary.scalar('mean', mean)  # 平均值
	        with tf.name_scope('stddev'):
	            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
	        tf.summary.scalar('stddev', stddev)  # 标准差
	        tf.summary.scalar('max', tf.reduce_max(var))  # 最大值
	        tf.summary.scalar('min', tf.reduce_min(var))  # 最小值
	        tf.summary.histogram('histogram', var)  # 直方图
	
	
	# 命名空间
	with tf.name_scope('input'):
	    # 这里的none表示第一个维度可以是任意的长度
	    x = tf.placeholder(tf.float32, [None, 784], name='x-input')
	    # 正确的标签
	    y = tf.placeholder(tf.float32, [None, 10], name='y-input')
	
	# 显示图片
	with tf.name_scope('input_reshape'):
	    image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])
	    tf.summary.image('input', image_shaped_input, 10)
	
	with tf.name_scope('layer'):
	    # 创建一个简单神经网络
	    with tf.name_scope('weights'):
	        W = tf.Variable(tf.zeros([784, 10]), name='W')
	        variable_summaries(W)
	    with tf.name_scope('biases'):
	        b = tf.Variable(tf.zeros([10]), name='b')
	        variable_summaries(b)
	    with tf.name_scope('wx_plus_b'):
	        wx_plus_b = tf.matmul(x, W) + b
	    with tf.name_scope('softmax'):
	        prediction = tf.nn.softmax(wx_plus_b)
	
	with tf.name_scope('loss'):
	    # 交叉熵代价函数
	    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))
	    tf.summary.scalar('loss', loss)
	with tf.name_scope('train'):
	    # 使用梯度下降法
	    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
	
	# 初始化变量
	sess.run(tf.global_variables_initializer())
	
	with tf.name_scope('accuracy'):
	    with tf.name_scope('correct_prediction'):
	        # 结果存放在一个布尔型列表中
	        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))  # argmax返回一维张量中最大的值所在的位置
	    with tf.name_scope('accuracy'):
	        # 求准确率
	        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  # 把correct_prediction变为float32类型
	        tf.summary.scalar('accuracy', accuracy)
	
	# 产生metadata文件
	if tf.gfile.Exists(DIR + 'projector/projector/metadata.tsv'):
	    tf.gfile.DeleteRecursively(DIR + 'projector/projector/metadata.tsv')
	with open(DIR + 'projector/projector/metadata.tsv', 'w') as f:
	    labels = sess.run(tf.argmax(mnist.test.labels[:], 1))
	    for i in range(image_num):
	        f.write(str(labels[i]) + '\n')
	
	        # 合并所有的summary
	merged = tf.summary.merge_all()
	
	projector_writer = tf.summary.FileWriter(DIR + 'projector/projector', sess.graph)
	saver = tf.train.Saver()
	config = projector.ProjectorConfig()
	embed = config.embeddings.add()
	embed.tensor_name = embedding.name
	embed.metadata_path = DIR + 'projector/projector/metadata.tsv'
	embed.sprite.image_path = DIR + 'projector/data/mnist_10k_sprite.png'
	embed.sprite.single_image_dim.extend([28, 28])
	projector.visualize_embeddings(projector_writer, config)
	
	for i in range(max_steps):
	    # 每个批次100个样本
	    batch_xs, batch_ys = mnist.train.next_batch(100)
	    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
	    run_metadata = tf.RunMetadata()
	    summary, _ = sess.run([merged, train_step], feed_dict={x: batch_xs, y: batch_ys}, options=run_options,
	                          run_metadata=run_metadata)
	    projector_writer.add_run_metadata(run_metadata, 'step%03d' % i)
	    projector_writer.add_summary(summary, i)
	
	    if i % 100 == 0:
	        acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})
	        print("Iter " + str(i) + ", Testing Accuracy= " + str(acc))
	
	saver.save(sess, DIR + 'projector/projector/a_model.ckpt', global_step=max_steps)
	projector_writer.close()
	sess.close()

![](https://i.imgur.com/SmDGtAA.png)